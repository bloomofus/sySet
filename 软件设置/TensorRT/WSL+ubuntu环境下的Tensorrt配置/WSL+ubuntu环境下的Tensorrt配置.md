# WSL+ubuntuç¯å¢ƒä¸‹çš„Tensorrté…ç½®

> å‚è€ƒç½‘å€ï¼šhttps://blog.csdn.net/qq_40102732/article/details/135182310
>
> å’Œå‚è€ƒçš„ç½‘å€æœ‰ä¸€ç‚¹å‡ºå…¥ï¼Œä½†æ˜¯å¤§å·®ä¸å·®

## NVIDIAé©±åŠ¨å®‰è£…

åœ¨WINä¸»æœºä¸Šç›´æ¥ä½¿ç”¨NVIDIA APPå®‰è£…å³å¯ï¼Œå®‰è£…WSLçš„ubuntuä¹‹åï¼Œåœ¨Linxuä½¿ç”¨`nvidia-smi`å‘ç°å¯ä»¥æ­£å¸¸æ˜¾ç¤ºå³å¯ï¼ŒWINä¸‹é¢ä½¿ç”¨è¿™ä¸ªå‘½ä»¤ä¹Ÿæ˜¯æ­£å¸¸çš„ã€‚

![image-20260130182343093](./WSL+ubuntu%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84Tensorrt%E9%85%8D%E7%BD%AE.assets/image-20260130182343093.png)

**è¦æ³¨æ„ï¼Œé©±åŠ¨æœ‰æ”¯æŒçš„cudaçš„æœ€é«˜ç‰ˆæœ¬ï¼Œåç»­å®‰è£…cuda toolkitã€cudnnã€tensorrtã€pytorchçš„ç‰ˆæœ¬å·éƒ½è¦ä¸€è‡´ï¼ï¼ï¼**

## cudaå®‰è£…

### å®‰è£…

ç”±äºé©±åŠ¨æ”¯æŒçš„æœ€é«˜cudaç‰ˆæœ¬æ˜¯12.9ï¼Œè€Œä¸”pytorchæ”¯æŒ12.8æ¯”è¾ƒå¤šï¼Œæ‰€ä»¥ï¼Œä¸‹é¢æˆ‘éƒ½ä»¥**cuda12.8**ä¸ºåŸºå‡†ã€‚

**æ³¨æ„ï¼ŒWSLçš„cudaå’Œæ™®é€šçš„linuxä¸ä¸€æ ·ï¼ŒNVIDIAå¼€å‘äº†ä¸“é—¨çš„cudaé©±åŠ¨ã€‚**

ç‚¹è¿›å¯¹åº”cuda toolkitç‰ˆæœ¬é€‰æ‹©çš„ç½‘å€:[CUDA Toolkit Archive | NVIDIA Developer](https://developer.nvidia.com/cuda-toolkit-archive)

![image-20260130182849350](./WSL+ubuntu%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84Tensorrt%E9%85%8D%E7%BD%AE.assets/image-20260130182849350.png)

![image-20260130183138826](./WSL+ubuntu%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84Tensorrt%E9%85%8D%E7%BD%AE.assets/image-20260130183138826.png)

æŒ‰ç…§æˆ‘çš„æ¡†æ¡†æ¥ï¼Œä¸èƒ½é€‰é”™äº†ï¼Œä¸‹é¢çš„å‘½ä»¤æ‰§è¡Œä¸€éå³å¯ï¼Œæ³¨æ„å½“å‰æ‰€åœ¨çš„æ–‡ä»¶å¤¹ï¼Œå®‰è£…åä¼šæ®‹ä½™ä¸€ä¸ªå®‰è£…åŒ…ã€‚

```bash
wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin
sudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/12.8.0/local_installers/cuda-repo-wsl-ubuntu-12-8-local_12.8.0-1_amd64.deb
sudo dpkg -i cuda-repo-wsl-ubuntu-12-8-local_12.8.0-1_amd64.deb
sudo cp /var/cuda-repo-wsl-ubuntu-12-8-local/cuda-*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt-get -y install cuda-toolkit-12-8
```

### éªŒè¯

ä¸‹é¢çš„æ–‡ä»¶å¤¹ä¼šå‡ºç°ï¼Œè¯´æ˜cudaå®‰è£…æˆåŠŸäº†ã€‚

![image-20260130183526182](./WSL+ubuntu%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84Tensorrt%E9%85%8D%E7%BD%AE.assets/image-20260130183526182.png)

### ç³»ç»Ÿç¯å¢ƒé…ç½®

```bash
# ç›´æ¥åœ¨ç»ˆç«¯è¿è¡Œ
sudo touch /etc/profile.d/cuda.sh
echo 'export PATH=/usr/local/cuda/bin/:$PATH' | sudo tee -a /etc/profile.d/cuda.sh
echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64/:/usr/lib/wsl/lib/:$LD_LIBRARY_PATH' | sudo tee -a /etc/profile.d/cuda.sh

# ä¸‹é¢çš„è¿½åŠ åˆ°~/.bashrcè¿™ä¸ªæ–‡ä»¶é‡Œé¢ï¼Œæœ€åå†source ~/.bashrcä¸€ä¸‹
export PATH=/usr/local/cuda/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
export PATH=/usr/local/cuda-12.8/bin${PATH:+:${PATH}}	# ä¿®æ”¹cudaç‰ˆæœ¬ä¸ºè‡ªå·±ä¸‹è½½çš„
export LD_LIBRARY_PATH=/usr/local/cuda-12.8/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}	# ä¿®æ”¹cudaç‰ˆæœ¬ä¸ºè‡ªå·±ä¸‹è½½çš„
export CUDA_HOME=/usr/local/cuda-12.8	# ä¿®æ”¹cudaç‰ˆæœ¬ä¸ºè‡ªå·±ä¸‹è½½çš„
```

### nvccéªŒè¯

ç»ˆç«¯è¾“å…¥`nvcc -V`ï¼Œå¦‚æœæ˜¾ç¤ºä¸‹é¢çš„æ ·å­åˆ™æ­£å¸¸å®‰è£…ã€‚

![image-20260130183916272](./WSL+ubuntu%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84Tensorrt%E9%85%8D%E7%BD%AE.assets/image-20260130183916272.png)

## cudnnå®‰è£…

### å®‰è£…

cudnné€‰æ‹©å¯¹åº”çš„taråŒ…è¿›è¡Œå®‰è£…ã€‚è¿›å…¥åˆ°cudnnçš„å®˜æ–¹ä¸‹è½½ç½‘å€:[Log in | NVIDIA Developer](https://developer.nvidia.com/rdp/cudnn-download)ï¼Œå‹¾é€‰I agreeï¼Œæœ€æ–°çš„ç‰ˆæœ¬å·²ç»åˆ°äº†9.18ï¼Œä½†æ˜¯æŒ‰ç…§åŸæ¥çš„æ•™ç¨‹ï¼Œæˆ‘è¿˜æ˜¯å®‰è£…8.9ç‰ˆæœ¬çš„ã€‚

è¿›å…¥å®˜ç½‘ï¼Œåˆ’åˆ°æœ€ä¸‹é¢ï¼Œé€‰æ‹©å¾€æœŸç‰ˆæœ¬ã€‚æˆ‘é€‰æ‹©8.9ç‰ˆæœ¬ï¼Œå¯ä»¥çœ‹åˆ°å…¶å¯¹cuda12.xç‰ˆæœ¬éƒ½é€‚é…ã€‚

![image-20260130184512769](./WSL+ubuntu%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84Tensorrt%E9%85%8D%E7%BD%AE.assets/image-20260130184512769.png)

![image-20260130184409726](./WSL+ubuntu%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84Tensorrt%E9%85%8D%E7%BD%AE.assets/image-20260130184409726.png)

![image-20260130184550189](./WSL+ubuntu%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84Tensorrt%E9%85%8D%E7%BD%AE.assets/image-20260130184550189.png)

å°†å…¶æ”¾å…¥ä¸€ä¸ªç‰¹å®šæ–‡ä»¶å¤¹ä¸‹ï¼Œæˆ‘æ”¾å…¥`/root/NVIDIA`ä¸‹é¢ã€‚ç„¶åæ‰§è¡Œä¸‹é¢çš„å‘½ä»¤ã€‚

```bash
tar -xvf cudnn-linux-x86_64-8.9.7.29_cuda12-archive.tar.xz	# æ ¹æ®ä¸‹è½½çš„åŒ…åè‡ªè¡Œä¿®æ”¹
sudo cp cudnn-*-archive/include/cudnn*.h /usr/local/cuda/include
sudo cp -P cudnn-*-archive/lib/libcudnn* /usr/local/cuda/lib64 
sudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn*
```

### éªŒè¯

éªŒè¯æ˜¯å¦å®‰è£…æˆåŠŸï¼Œè¾“å…¥å¦‚ä¸‹å‘½ä»¤ï¼Œæ˜¾ç¤ºå¦‚ä¸‹å›¾å³å®‰è£…æˆåŠŸã€‚

```bash
cat /usr/local/cuda/include/cudnn_version.h | grep CUDNN_MAJOR -A 2
```

![image-20260130185004806](./WSL+ubuntu%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84Tensorrt%E9%85%8D%E7%BD%AE.assets/image-20260130185004806.png)

å¯ä»¥çœ‹åˆ°æ˜¾ç¤ºçš„ç‰ˆæœ¬æ˜¯8.9ï¼Œè¿™ä¸ªæ˜¯cudnnçš„ç‰ˆæœ¬ï¼Œä¸taråŒ…æ˜¾ç¤ºä¸€è‡´ï¼Œå®‰è£…æˆåŠŸã€‚

## TensorRTå®‰è£…

TensorRTé€‰æ‹©å¯¹åº”çš„taråŒ…è¿›è¡Œå®‰è£…ï¼Œè¿›å…¥åˆ°TensorRTå®˜æ–¹ä¸‹è½½ç½‘å€[Log in | NVIDIA Developer](https://developer.nvidia.com/tensorrt/download)ï¼Œç‚¹å‡»å‹¾é€‰I agreeã€‚

å¯ä»¥çœ‹åˆ°æœ€æ–°ç‰ˆæœ¬å·²ç»åˆ°10äº†ï¼Œè¿™é‡Œæˆ‘ä»¬å®‰è£…çš„å°±æ˜¯æœ€æ–°ç‰ˆæœ¬10.15ã€‚

![image-20260130185404071](./WSL+ubuntu%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84Tensorrt%E9%85%8D%E7%BD%AE.assets/image-20260130185404071.png)

![image-20260130185512843](./WSL+ubuntu%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84Tensorrt%E9%85%8D%E7%BD%AE.assets/image-20260130185512843.png)

![image-20260130185527927](./WSL+ubuntu%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84Tensorrt%E9%85%8D%E7%BD%AE.assets/image-20260130185527927.png)

å°†ä¸‹è½½çš„åŒ…å¤åˆ¶åˆ°`/root/NVIDIA`æ–‡ä»¶å¤¹ä¸‹é¢ï¼Œç„¶åè§£å‹è¿›è¡Œé…ç½®ã€‚

```bash
tar -xzvf TensorRT-10.15.1.29.Linux.x86_64-gnu.cuda-12.9.tar.gz
```

è§£å‹ä¹‹åæœ‰å¦‚ä¸‹çš„æ–‡ä»¶å¤¹ï¼Œå…¶ä¸­pythonçš„æ–‡ä»¶å¤¹é‡Œé¢æœ‰ä¸‰ç§whlæ–‡ä»¶ï¼Œå¦‚æœä½ çš„pyé¡¹ç›®éœ€è¦ç”¨åˆ°TensorRTï¼Œé‚£ä¹ˆä½ è¿˜éœ€è¦å®‰è£…whlä¾èµ–ï¼Œå¦‚æœä½ çš„é¡¹ç›®æ˜¯cppï¼Œé‚£ä¹ˆæŠŠå®ƒè§£å‹å¥½æ”¾è¿™ä¸åŠ¨å°±è¡Œã€‚ï¼ˆå‡è£…è¿™é‡Œçš„tmpæ–‡ä»¶å¤¹æ˜¯NVIDIAæ–‡ä»¶å¤¹ï¼‰

![image-20260130185739696](./WSL+ubuntu%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84Tensorrt%E9%85%8D%E7%BD%AE.assets/image-20260130185739696.png)

å°†TensorRT ä¸‹çš„libç»å¯¹è·¯å¾„æ·»åŠ åˆ°ç³»ç»Ÿç¯å¢ƒä¸­(æ ¹æ®è‡ªå·±çš„å®‰è£…ç›®å½•æ¥)

```
# é€šè¿‡nano ~/.bashrcæŠŠä¸‹é¢å‘½ä»¤è¿½åŠ åˆ°å¯åŠ¨è„šæœ¬ï¼Œå®ç°å°†tensorrtçš„åº“åŠ å…¥ç³»ç»Ÿç¯å¢ƒå˜é‡ï¼Œå†source ~/.bashrc
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/root/NVIDIA/TensorRT-10.15.1.29/lib
```

###  è§£å‹åçš„ç›®å½•ç»“æ„

TensorRT è§£å‹åçš„ç›®å½•ç»“æ„æ˜¯æ ‡å‡†çš„ SDK å¸ƒå±€ï¼Œå„ç›®å½•ä½œç”¨å¦‚ä¸‹ï¼š

| ç›®å½•           | ä½œç”¨                                                         | é‡è¦æ€§            |
| -------------- | ------------------------------------------------------------ | ----------------- |
| **`bin/`**     | å‘½ä»¤è¡Œå·¥å…·ç›®å½•ï¼ŒåŒ…å« `trtexec` ç­‰å®ç”¨ç¨‹åºã€‚`trtexec` å¯ç›´æ¥åŠ è½½ ONNX/UFF æ¨¡å‹è¿›è¡Œæ¨ç†æµ‹è¯•ï¼Œæ— éœ€ç¼–å†™ä»£ç  [[4]] | â­ å¸¸ç”¨å·¥å…·        |
| **`doc/`**     | å®˜æ–¹æ–‡æ¡£ï¼ŒåŒ…å« API å‚è€ƒã€å¼€å‘æŒ‡å—ã€å‘è¡Œè¯´æ˜ç­‰ PDF/HTML æ ¼å¼æ–‡æ¡£ | ğŸ“š å‚è€ƒèµ„æ–™        |
| **`include/`** | C++ å¤´æ–‡ä»¶ï¼ˆ`.h`ï¼‰ï¼Œç”¨äº C++ å¼€å‘æ—¶ `#include <NvInfer.h>` ç­‰ã€‚ç¼–è¯‘ C++ é¡¹ç›®æ—¶éœ€æŒ‡å®šæ­¤è·¯å¾„ä¸º `-I` å‚æ•° | âš™ï¸ C++ å¼€å‘å¿…éœ€    |
| **`lib/`**     | **æ ¸å¿ƒå…±äº«åº“ç›®å½•**ï¼ŒåŒ…å« `libnvinfer.so`ã€`libnvinfer_plugin.so` ç­‰ TensorRT è¿è¡Œæ—¶åº“ã€‚Python/C++ ç¨‹åºè¿è¡Œæ—¶éœ€é€šè¿‡ `LD_LIBRARY_PATH` åŠ è½½æ­¤ç›®å½• [[5]] | âš ï¸ **è¿è¡Œæ—¶å¿…éœ€**  |
| **`python/`**  | Python wheel åŒ…ï¼ˆ`.whl`ï¼‰ï¼ŒåŒ…å« `tensorrt`ã€`tensorrt_lean`ã€`tensorrt_dispatch` ä¸‰ä¸ªç»„ä»¶ï¼Œç”¨äº Python API è°ƒç”¨ | ğŸ Python å¼€å‘å¿…éœ€ |
| **`targets/`** | **å¤šæ¶æ„æ”¯æŒç›®å½•**ï¼ŒåŒ…å«ä¸åŒç¡¬ä»¶å¹³å°çš„åº“æ–‡ä»¶ï¼ˆå¦‚ `x86_64-linux-gnu`ã€`aarch64-linux-gnu` ç­‰ï¼‰ã€‚åœ¨äº¤å‰ç¼–è¯‘æˆ–åµŒå…¥å¼å¼€å‘ï¼ˆå¦‚ Jetsonï¼‰æ—¶ä½¿ç”¨ [[22]] | ğŸ¯ ç‰¹å®šåœºæ™¯éœ€è¦    |

**å…³é”®ä½¿ç”¨è¯´æ˜**

1. **è¿è¡Œæ—¶ä¾èµ–**  
   å³ä½¿é€šè¿‡ pip/pixi å®‰è£…äº† Python wheelï¼Œ**ä»éœ€å°† `lib/` ç›®å½•åŠ å…¥ç³»ç»Ÿåº“è·¯å¾„**ï¼Œå¦åˆ™ä¼šæŠ¥ `libnvinfer.so: cannot open shared object file` é”™è¯¯ï¼š
   
   ```bash
   export LD_LIBRARY_PATH=/path/to/TensorRT-10.15.1.29/lib:$LD_LIBRARY_PATH
   ```
   
2. **C++ å¼€å‘é…ç½®**  
   ç¼–è¯‘ C++ é¡¹ç›®æ—¶éœ€æŒ‡å®šï¼š
   ```bash
   -I/path/to/TensorRT/include    # å¤´æ–‡ä»¶è·¯å¾„
   -L/path/to/TensorRT/lib        # åº“æ–‡ä»¶è·¯å¾„
   -lnvinfer -lnvinfer_plugin     # é“¾æ¥åº“
   ```

3. **`targets/` ç›®å½•çš„ç‰¹æ®Šæ€§**  
   - åœ¨ x86_64 æœåŠ¡å™¨/PC ä¸Šï¼Œé€šå¸¸ç›´æ¥ä½¿ç”¨æ ¹ç›®å½•çš„ `lib/` å’Œ `include/`
   - åœ¨ Jetson ç­‰ ARM è®¾å¤‡ä¸Šï¼Œå¯èƒ½éœ€è¦ä» `targets/aarch64-linux-gnu/` å¤åˆ¶å¯¹åº”æ–‡ä»¶åˆ°ç³»ç»Ÿè·¯å¾„

4. **å®Œæ•´éƒ¨ç½²å»ºè®®**  
   ç”Ÿäº§ç¯å¢ƒä¸­å»ºè®®å°† `lib/` ä¸‹çš„ `.so` æ–‡ä»¶å¤åˆ¶åˆ°ç³»ç»Ÿåº“ç›®å½•ï¼ˆå¦‚ `/usr/local/lib`ï¼‰ï¼Œå¹¶å°† `include/` å¤´æ–‡ä»¶å¤åˆ¶åˆ° `/usr/local/include`ï¼Œç„¶åè¿è¡Œ `ldconfig` åˆ·æ–°ç¼“å­˜ã€‚

### cppç¯å¢ƒå®‰è£…

ç•¥

### pyç¯å¢ƒå®‰è£…

TensorRTè§£å‹ä¹‹åæ˜¯ä¸‹é¢çš„æ–‡ä»¶ã€‚å…¶ä¸­cp311æ˜¯pyç‰ˆæœ¬å·ï¼Œä½ çš„pyç¯å¢ƒæ˜¯ä»€ä¹ˆå°±å®‰è£…å“ªä¸€ä¸ªã€‚

![image-20260130190422297](./WSL+ubuntu%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84Tensorrt%E9%85%8D%E7%BD%AE.assets/image-20260130190422297.png)

#### pipæˆ–å…¶ä»–condaç¯å¢ƒï¼ˆæœªæµ‹è¯•ï¼‰

ä½¿ç”¨pipå‘½ä»¤å³å¯å®‰è£…ã€‚

```bash
cd python
pip install tensorrt-10.15.1.29-cp311-none-linux_x86_64.whl
pip install tensorrt_dispatch-10.15.1.29-cp311-none-linux_x86_64.whl
pip install tensorrt_lean-10.15.1.29-cp311-none-linux_x86_64.whl
```

ä¹‹åï¼Œè¿™ä¸ªç¯å¢ƒå°±å¯ä»¥ä½¿ç”¨TensorRTäº†ã€‚

#### pixiç¯å¢ƒ

éœ€è¦å‚è€ƒä¸‹é¢çš„å‘½ä»¤æ¥è¿›è¡Œå®‰è£…ã€‚

```bash
cd ~/NVIDIA/TensorRT-10.15.1.29/python
# å®‰è£… lean
pixi add --pypi "tensorrt_lean @ file://$(pwd)/tensorrt_lean-10.15.1.29-cp311-none-linux_x86_64.whl"
# å®‰è£… dispatch
pixi add --pypi "tensorrt_dispatch @ file://$(pwd)/tensorrt_dispatch-10.15.1.29-cp311-none-linux_x86_64.whl"
# å®‰è£…ä¸»åŒ…
pixi add --pypi "tensorrt @ file://$(pwd)/tensorrt-10.15.1.29-cp311-none-linux_x86_64.whl"
```

### cppæ‰§è¡Œæµ‹è¯•

test_tensorrt.cppä»£ç å¦‚ä¸‹ï¼š

```cpp
#include <iostream>
#include <NvInfer.h>
#include <NvInferRuntime.h>
#include <cuda_runtime.h>

// TensorRT Logger
class Logger : public nvinfer1::ILogger {
public:
    void log(Severity severity, const char* msg) noexcept override {
        if (severity <= Severity::kWARNING) {
            std::cout << "[TensorRT] " << msg << std::endl;
        }
    }
};

int main() {
    std::cout << "========== TensorRT 10 éªŒè¯æµ‹è¯• ==========" << std::endl;
    
    // 1. æ‰“å°ç‰ˆæœ¬ä¿¡æ¯
    std::cout << "\n[1] ç‰ˆæœ¬ä¿¡æ¯:" << std::endl;
    std::cout << "  TensorRT ç‰ˆæœ¬: " << NV_TENSORRT_MAJOR << "." 
              << NV_TENSORRT_MINOR << "." 
              << NV_TENSORRT_PATCH << std::endl;
    
    // æ£€æŸ¥ CUDA
    int cudaVersion;
    cudaRuntimeGetVersion(&cudaVersion);
    std::cout << "  CUDA Runtime ç‰ˆæœ¬: " << cudaVersion / 1000 << "." 
              << (cudaVersion % 1000) / 10 << std::endl;
    
    // æ£€æŸ¥ GPU
    int deviceCount;
    cudaGetDeviceCount(&deviceCount);
    std::cout << "  å¯ç”¨ GPU æ•°é‡: " << deviceCount << std::endl;
    
    if (deviceCount > 0) {
        cudaDeviceProp prop;
        cudaGetDeviceProperties(&prop, 0);
        std::cout << "  GPU 0: " << prop.name << std::endl;
    }
    
    // 2. åˆ›å»º Builder
    std::cout << "\n[2] åˆ›å»º TensorRT Builder..." << std::endl;
    Logger logger;
    nvinfer1::IBuilder* builder = nvinfer1::createInferBuilder(logger);
    if (!builder) {
        std::cerr << "âŒ åˆ›å»º Builder å¤±è´¥!" << std::endl;
        return -1;
    }
    std::cout << "  âœ… Builder åˆ›å»ºæˆåŠŸ" << std::endl;
    
    // 3. åˆ›å»ºç½‘ç»œå®šä¹‰ (TensorRT 10.x æ–°æ–¹å¼ - ä¸éœ€è¦ flags)
    std::cout << "\n[3] åˆ›å»ºç½‘ç»œå®šä¹‰..." << std::endl;
    nvinfer1::INetworkDefinition* network = builder->createNetworkV2(0);
    if (!network) {
        std::cerr << "âŒ åˆ›å»ºç½‘ç»œå¤±è´¥!" << std::endl;
        return -1;
    }
    std::cout << "  âœ… ç½‘ç»œå®šä¹‰åˆ›å»ºæˆåŠŸ" << std::endl;
    
    // 4. æ·»åŠ ç®€å•çš„è¾“å…¥å±‚
    std::cout << "\n[4] æ·»åŠ ç½‘ç»œå±‚..." << std::endl;
    nvinfer1::ITensor* input = network->addInput(
        "input", nvinfer1::DataType::kFLOAT, nvinfer1::Dims4{1, 3, 224, 224});
    if (!input) {
        std::cerr << "âŒ æ·»åŠ è¾“å…¥å¤±è´¥!" << std::endl;
        return -1;
    }
    std::cout << "  âœ… è¾“å…¥å±‚æ·»åŠ æˆåŠŸ: [1, 3, 224, 224]" << std::endl;
    
    // 5. æ·»åŠ  Identity å±‚ (ç®€å•æµ‹è¯•)
    nvinfer1::IIdentityLayer* identity = network->addIdentity(*input);
    if (!identity) {
        std::cerr << "âŒ æ·»åŠ  Identity å±‚å¤±è´¥!" << std::endl;
        return -1;
    }
    identity->getOutput(0)->setName("output");
    network->markOutput(*identity->getOutput(0));
    std::cout << "  âœ… Identity å±‚æ·»åŠ æˆåŠŸ" << std::endl;
    
    // 6. åˆ›å»ºæ„å»ºé…ç½®
    std::cout << "\n[5] åˆ›å»ºæ„å»ºé…ç½®..." << std::endl;
    nvinfer1::IBuilderConfig* config = builder->createBuilderConfig();
    if (!config) {
        std::cerr << "âŒ åˆ›å»ºé…ç½®å¤±è´¥!" << std::endl;
        return -1;
    }
    
    // è®¾ç½®å†…å­˜æ± é™åˆ¶
    config->setMemoryPoolLimit(nvinfer1::MemoryPoolType::kWORKSPACE, 1ULL << 30);
    std::cout << "  âœ… é…ç½®åˆ›å»ºæˆåŠŸ (å·¥ä½œç©ºé—´: 1GB)" << std::endl;
    
    // 7. æ„å»ºå¼•æ“
    std::cout << "\n[6] æ„å»ºåºåˆ—åŒ–å¼•æ“..." << std::endl;
    nvinfer1::IHostMemory* serializedEngine = builder->buildSerializedNetwork(*network, *config);
    if (!serializedEngine) {
        std::cerr << "âŒ æ„å»ºå¼•æ“å¤±è´¥!" << std::endl;
        return -1;
    }
    std::cout << "  âœ… å¼•æ“æ„å»ºæˆåŠŸ! å¤§å°: " << serializedEngine->size() << " bytes" << std::endl;
    
    // 8. åˆ›å»ºè¿è¡Œæ—¶å¹¶ååºåˆ—åŒ–
    std::cout << "\n[7] åˆ›å»ºè¿è¡Œæ—¶..." << std::endl;
    nvinfer1::IRuntime* runtime = nvinfer1::createInferRuntime(logger);
    if (!runtime) {
        std::cerr << "âŒ åˆ›å»ºè¿è¡Œæ—¶å¤±è´¥!" << std::endl;
        return -1;
    }
    
    nvinfer1::ICudaEngine* engine = runtime->deserializeCudaEngine(
        serializedEngine->data(), serializedEngine->size());
    if (!engine) {
        std::cerr << "âŒ ååºåˆ—åŒ–å¼•æ“å¤±è´¥!" << std::endl;
        return -1;
    }
    std::cout << "  âœ… å¼•æ“ååºåˆ—åŒ–æˆåŠŸ" << std::endl;
    
    // 9. åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡
    std::cout << "\n[8] åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡..." << std::endl;
    nvinfer1::IExecutionContext* context = engine->createExecutionContext();
    if (!context) {
        std::cerr << "âŒ åˆ›å»ºä¸Šä¸‹æ–‡å¤±è´¥!" << std::endl;
        return -1;
    }
    std::cout << "  âœ… æ‰§è¡Œä¸Šä¸‹æ–‡åˆ›å»ºæˆåŠŸ" << std::endl;
    
    // 10. æ‰“å°ç»‘å®šä¿¡æ¯ (TensorRT 10 æ–° API)
    std::cout << "\n[9] ç»‘å®šä¿¡æ¯:" << std::endl;
    int numIOTensors = engine->getNbIOTensors();
    for (int i = 0; i < numIOTensors; i++) {
        const char* name = engine->getIOTensorName(i);
        nvinfer1::TensorIOMode mode = engine->getTensorIOMode(name);
        nvinfer1::Dims dims = engine->getTensorShape(name);
        
        std::cout << "  [" << i << "] " << name 
                  << " (" << (mode == nvinfer1::TensorIOMode::kINPUT ? "INPUT" : "OUTPUT") << ")"
                  << " å½¢çŠ¶: [";
        for (int j = 0; j < dims.nbDims; j++) {
            std::cout << dims.d[j];
            if (j < dims.nbDims - 1) std::cout << ", ";
        }
        std::cout << "]" << std::endl;
    }
    
    // 11. ç®€å•æ¨ç†æµ‹è¯•
    std::cout << "\n[10] æ‰§è¡Œæ¨ç†æµ‹è¯•..." << std::endl;
    
    // åˆ†é… GPU å†…å­˜
    float* d_input;
    float* d_output;
    size_t inputSize = 1 * 3 * 224 * 224 * sizeof(float);
    size_t outputSize = 1 * 3 * 224 * 224 * sizeof(float);
    
    cudaMalloc(&d_input, inputSize);
    cudaMalloc(&d_output, outputSize);
    
    // è®¾ç½®è¾“å…¥è¾“å‡ºåœ°å€ (TensorRT 10 æ–° API)
    context->setTensorAddress("input", d_input);
    context->setTensorAddress("output", d_output);
    
    // åˆ›å»º CUDA æµ
    cudaStream_t stream;
    cudaStreamCreate(&stream);
    
    // æ‰§è¡Œæ¨ç†
    bool status = context->enqueueV3(stream);
    cudaStreamSynchronize(stream);
    
    if (status) {
        std::cout << "  âœ… æ¨ç†æ‰§è¡ŒæˆåŠŸ!" << std::endl;
    } else {
        std::cerr << "  âŒ æ¨ç†æ‰§è¡Œå¤±è´¥!" << std::endl;
    }
    
    // æ¸…ç† CUDA èµ„æº
    cudaFree(d_input);
    cudaFree(d_output);
    cudaStreamDestroy(stream);
    
    // æ¸…ç† TensorRT èµ„æº
    std::cout << "\n[11] æ¸…ç†èµ„æº..." << std::endl;
    delete context;
    delete engine;
    delete runtime;
    delete serializedEngine;
    delete config;
    delete network;
    delete builder;
    
    std::cout << "\n========================================" << std::endl;
    std::cout << "âœ… TensorRT 10 C++ éªŒè¯å®Œæˆ!" << std::endl;
    std::cout << "========================================" << std::endl;
    
    return 0;
}

```

CMakeLists.txtå†…å®¹å¦‚ä¸‹ï¼š

```cmake
cmake_minimum_required(VERSION 3.18)
project(tensorrt10_test LANGUAGES CXX CUDA)

set(CMAKE_CXX_STANDARD 17)
set(CMAKE_CXX_STANDARD_REQUIRED ON)

# è®¾ç½® TensorRT è·¯å¾„
set(TENSORRT_ROOT "/root/NVIDIA/TensorRT-10.15.1.29" CACHE PATH "TensorRT root")

# æŸ¥æ‰¾ CUDA Toolkit (æ­£ç¡®æ–¹å¼)
find_package(CUDAToolkit REQUIRED)

# æ‰“å°è°ƒè¯•ä¿¡æ¯
message(STATUS "CUDA Toolkit ç‰ˆæœ¬: ${CUDAToolkit_VERSION}")
message(STATUS "CUDA åº“è·¯å¾„: ${CUDAToolkit_LIBRARY_DIR}")
message(STATUS "TensorRT è·¯å¾„: ${TENSORRT_ROOT}")

# å¤´æ–‡ä»¶è·¯å¾„
include_directories(
    ${CUDAToolkit_INCLUDE_DIRS}
    ${TENSORRT_ROOT}/include
)

# TensorRT åº“è·¯å¾„
link_directories(
    ${TENSORRT_ROOT}/lib
)

# åˆ›å»ºå¯æ‰§è¡Œæ–‡ä»¶
add_executable(test_tensorrt test_tensorrt.cpp)

# é“¾æ¥åº“
target_link_libraries(test_tensorrt
    nvinfer
    CUDA::cudart
)

```

ç¼–è¯‘å‘½ä»¤å¦‚ä¸‹ï¼š

```bash
# æ¸…ç†æ—§çš„æ„å»º
cd /root/NVIDIA/test/cppTest
rm -rf build
mkdir build && cd build
# é‡æ–°é…ç½®
cmake .. -DTENSORRT_ROOT=/root/NVIDIA/TensorRT-10.15.1.29
# ç¼–è¯‘
make -j$(nproc)
# è¿è¡Œ
./test_tensorrt
```

æ­£å¸¸è¿è¡Œç»“æœå¦‚ä¸‹ï¼š

![image-20260130191555276](./WSL+ubuntu%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84Tensorrt%E9%85%8D%E7%BD%AE.assets/image-20260130191555276.png)

### pyæ‰§è¡Œæµ‹è¯•

```bash
# ä¸‹è½½æµ‹è¯•ç”¨ ONNX æ¨¡å‹ï¼ˆä»¥ ResNet50 ä¸ºä¾‹ï¼‰
wget https://github.com/onnx/models/raw/main/vision/classification/resnet/model/resnet50-v2-7.onnx -O /tmp/resnet50.onnx
```

testTRT.pyä»£ç å¦‚ä¸‹ï¼š

```python
#!/usr/bin/env python3
"""
TensorRT 10 å®‰è£…éªŒè¯è„šæœ¬
æµ‹è¯•æ¨¡å‹: /tmp/resnet50.onnx
"""

import sys
import numpy as np

def check_tensorrt():
    """æ£€æŸ¥TensorRTå®‰è£…"""
    print("=" * 50)
    print("TensorRT 10 å®‰è£…éªŒè¯")
    print("=" * 50)
    
    # 1. å¯¼å…¥æ£€æŸ¥
    print("\n[1/5] æ£€æŸ¥TensorRTå¯¼å…¥...")
    try:
        import tensorrt as trt
        print(f"  âœ“ TensorRT å¯¼å…¥æˆåŠŸ")
        print(f"  âœ“ TensorRT ç‰ˆæœ¬: {trt.__version__}")
    except ImportError as e:
        print(f"  âœ— TensorRT å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    # 2. æ£€æŸ¥CUDA
    print("\n[2/5] æ£€æŸ¥CUDAæ”¯æŒ...")
    try:
        import pycuda.driver as cuda
        import pycuda.autoinit
        print(f"  âœ“ PyCUDA å¯ç”¨")
        print(f"  âœ“ CUDA è®¾å¤‡: {cuda.Device(0).name()}")
    except Exception as e:
        print(f"  ! PyCUDA ä¸å¯ç”¨ (å¯é€‰): {e}")
    
    # 3. åˆ›å»ºLoggerå’ŒBuilder
    print("\n[3/5] åˆ›å»ºTensorRTç»„ä»¶...")
    try:
        logger = trt.Logger(trt.Logger.WARNING)
        builder = trt.Builder(logger)
        print(f"  âœ“ Logger åˆ›å»ºæˆåŠŸ")
        print(f"  âœ“ Builder åˆ›å»ºæˆåŠŸ")
        print(f"  âœ“ æœ€å¤§çº¿ç¨‹æ•°: {builder.max_threads}")
    except Exception as e:
        print(f"  âœ— ç»„ä»¶åˆ›å»ºå¤±è´¥: {e}")
        return False
    
    # 4. è§£æONNXæ¨¡å‹
    print("\n[4/5] è§£æONNXæ¨¡å‹...")
    onnx_path = "/tmp/resnet50.onnx"
    
    try:
        network = builder.create_network(
            1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
        )
        parser = trt.OnnxParser(network, logger)
        
        with open(onnx_path, 'rb') as f:
            if not parser.parse(f.read()):
                print(f"  âœ— ONNXè§£æå¤±è´¥:")
                for i in range(parser.num_errors):
                    print(f"    - {parser.get_error(i)}")
                return False
        
        print(f"  âœ“ ONNXæ¨¡å‹è§£ææˆåŠŸ: {onnx_path}")
        print(f"  âœ“ ç½‘ç»œè¾“å…¥æ•°: {network.num_inputs}")
        print(f"  âœ“ ç½‘ç»œè¾“å‡ºæ•°: {network.num_outputs}")
        
        for i in range(network.num_inputs):
            inp = network.get_input(i)
            print(f"    è¾“å…¥[{i}]: {inp.name}, shape={inp.shape}, dtype={inp.dtype}")
        for i in range(network.num_outputs):
            out = network.get_output(i)
            print(f"    è¾“å‡º[{i}]: {out.name}, shape={out.shape}, dtype={out.dtype}")
            
    except FileNotFoundError:
        print(f"  âœ— æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {onnx_path}")
        return False
    except Exception as e:
        print(f"  âœ— æ¨¡å‹è§£æå¤±è´¥: {e}")
        return False
    
    # 5. æ„å»ºå¼•æ“
    print("\n[5/5] æ„å»ºTensorRTå¼•æ“...")
    try:
        config = builder.create_builder_config()
        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)
        
        # æ·»åŠ ä¼˜åŒ–é…ç½®æ–‡ä»¶å¤„ç†åŠ¨æ€ç»´åº¦
        profile = builder.create_optimization_profile()
        
        for i in range(network.num_inputs):
            inp = network.get_input(i)
            inp_name = inp.name
            inp_shape = inp.shape
            
            if -1 in inp_shape:
                print(f"  - æ£€æµ‹åˆ°åŠ¨æ€è¾“å…¥: {inp_name}, shape={inp_shape}")
                
                min_shape = tuple(1 if d == -1 else d for d in inp_shape)
                opt_shape = tuple(4 if d == -1 else d for d in inp_shape)
                max_shape = tuple(16 if d == -1 else d for d in inp_shape)
                
                print(f"    min_shape: {min_shape}")
                print(f"    opt_shape: {opt_shape}")
                print(f"    max_shape: {max_shape}")
                
                profile.set_shape(inp_name, min_shape, opt_shape, max_shape)
        
        config.add_optimization_profile(profile)
        
        if builder.platform_has_fast_fp16:
            print(f"  âœ“ å¹³å°æ”¯æŒFP16åŠ é€Ÿ")
        
        print(f"  - æ­£åœ¨æ„å»ºå¼•æ“ (å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
        
        serialized_engine = builder.build_serialized_network(network, config)
        
        if serialized_engine is None:
            print(f"  âœ— å¼•æ“æ„å»ºå¤±è´¥")
            return False
        
        # ============== ä¿®å¤: ä½¿ç”¨ .nbytes è€Œä¸æ˜¯ len() ==============
        engine_size = serialized_engine.nbytes
        print(f"  âœ“ å¼•æ“æ„å»ºæˆåŠŸ")
        print(f"  âœ“ åºåˆ—åŒ–å¼•æ“å¤§å°: {engine_size / 1024 / 1024:.2f} MB")
        
        # ååºåˆ—åŒ–æµ‹è¯•
        runtime = trt.Runtime(logger)
        engine = runtime.deserialize_cuda_engine(serialized_engine)
        
        if engine is None:
            print(f"  âœ— å¼•æ“ååºåˆ—åŒ–å¤±è´¥")
            return False
            
        print(f"  âœ“ å¼•æ“ååºåˆ—åŒ–æˆåŠŸ")
        print(f"  âœ“ å¼•æ“å±‚æ•°: {engine.num_layers}")
        
        # å¯é€‰: ä¿å­˜å¼•æ“åˆ°æ–‡ä»¶
        # with open("/tmp/resnet50.engine", "wb") as f:
        #     f.write(serialized_engine)
        # print(f"  âœ“ å¼•æ“å·²ä¿å­˜åˆ°: /tmp/resnet50.engine")
        
    except Exception as e:
        print(f"  âœ— å¼•æ“æ„å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # å®Œæˆ
    print("\n" + "=" * 50)
    print("âœ“ TensorRT 10 å®‰è£…éªŒè¯é€šè¿‡!")
    print("=" * 50)
    return True


if __name__ == "__main__":
    success = check_tensorrt()
    sys.exit(0 if success else 1)

```

æ­£ç¡®æ‰§è¡Œç»“æœå¦‚ä¸‹ï¼š

![image-20260130191943495](./WSL+ubuntu%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84Tensorrt%E9%85%8D%E7%BD%AE.assets/image-20260130191943495.png)

## å¸è½½ï¼ˆå…¨éƒ½æœªæµ‹è¯•ï¼‰

### cudaå¸è½½

```bash
sudo apt-get --purge remove "*cuda*" "*cublas*" "*cufft*" "*cufile*" "*curand*" \
 "*cusolver*" "*cusparse*" "*gds-tools*" "*npp*" "*nvjpeg*" "nsight*" 
sudo apt-get autoremove
```

### cudnnå¸è½½

```bash
sudo apt-get purge libcudnn8
sudo apt-get purge libcudnn8-dev
 
# or
sudo apt-get --purge remove "*cublas*" "*cufft*" "*curand*" "*cusolver*" "*cusparse*" "*npp*" "*nvjpeg*" "cuda*" "nsight*" 
 
cd /usr/local/cuda-xx.x/bin/
sudo ./cuda-uninstaller
sudo rm -rf /usr/local/cuda-xx.x
 
# è‹¥å®‰è£…äº†cudaåªæ˜¯æƒ³å¸è½½cudnn, (xx.xä¸ºå®‰è£…çš„cudaç‰ˆæœ¬)
sudo rm /usr/local/cuda-xx.x/include/cudnn*
sudo rm /usr/local/cuda-xx.x/lib64/libcudnn*
```

### TensorRTå¸è½½

```bash
#ï¼ˆå› ä¸ºæœ‰å¯èƒ½ä¸‹è½½pythonå¯¹ç”¨çš„tensorrtï¼Œæ‰€ä»¥ä¼šæœ‰pipçš„å¸è½½)
sudo apt-get purge "libnvinfer*"
sudo apt-get purge "nv-tensorrt-repo*"
sudo apt-get purge graphsurgeon-tf onnx-graphsurgeon
pip3 uninstall tensorrt
pip3 uninstall uff
pip3 uninstall graphsurgeon
pip3 uninstall onnx-graphsurgeon
python3 -m pip uninstall nvidia-tensorrt
```

